### ðŸ”¹ 1. `nvidia-smi` â€“ Core Command

`nvidia-smi` is the most important NVIDIA tool to monitor and manage GPUs.

* **Basic Monitoring**

  * `nvidia-smi` â†’ Show GPU summary (processes, usage, memory, driver version, etc.)
  * `nvidia-smi -L` â†’ List GPUs with UUIDs
  * `nvidia-smi -q` â†’ Detailed GPU information (full query)
  * `nvidia-smi -q -d MEMORY` â†’ Show only memory-related details
  * `nvidia-smi -q -d POWER` â†’ Show only power usage details
  * `nvidia-smi dmon` â†’ Live monitoring (similar to `top` for GPUs)
  * `nvidia-smi pmon -i 0` â†’ Live per-process GPU usage

* **Monitoring Options**

  * `nvidia-smi -i <id>` â†’ Select a specific GPU
  * `nvidia-smi --query-gpu=utilization.gpu,utilization.memory --format=csv`
    â†’ Custom queries (usage in CSV format)
  * `nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv`
    â†’ See which processes use how much GPU memory
  * `nvidia-smi -l 2` â†’ Refresh every 2 seconds

* **Process Management**

  * `nvidia-smi pmon` â†’ Monitor processes using GPU
  * `nvidia-smi --gpu-reset -i <id>` â†’ Reset a GPU (if no processes running)
  * `nvidia-smi --kill-pid=<pid>` â†’ Kill a process using GPU

* **Power & Performance**

  * `nvidia-smi --persistence-mode=1` â†’ Enable persistence mode (keep driver loaded)
  * `nvidia-smi -pm 1` â†’ Same as above
  * `nvidia-smi --auto-boost-default=0` â†’ Disable auto boost
  * `nvidia-smi -pl <watts>` â†’ Set power limit
  * `nvidia-smi -ac <memClock,graphicsClock>` â†’ Set application clocks
  * `nvidia-smi -rac` â†’ Reset application clocks

* **Driver / Version Info**

  * `nvidia-smi --query-gpu=driver_version,vbios_version --format=csv`
  * `nvidia-smi -q -d SUPPORTED_CLOCKS` â†’ See supported clock speeds

---

### ðŸ”¹ 2. NVIDIA CUDA Tools (if CUDA toolkit installed)

* `nvcc --version` â†’ Check CUDA compiler version
* `deviceQuery` (in CUDA samples) â†’ Show GPU compute capability & features
* `bandwidthTest` (in CUDA samples) â†’ Measure memory bandwidth

---

### ðŸ”¹ 3. NVIDIA NVML (C Library, for developers)

* `nvml` is not a direct command but a library that powers `nvidia-smi`.
  Developers can use it in Python/C/C++ to query GPU metrics.

---

### ðŸ”¹ 4. NVIDIA Debugging & Profiling Tools

* `cuda-gdb` â†’ Debug CUDA applications
* `cuda-memcheck` â†’ Detect memory errors in CUDA programs
* `nsys` (NVIDIA Nsight Systems) â†’ System-wide performance profiling
* `nvprof` (deprecated, replaced by Nsight) â†’ GPU profiling

---

### ðŸ”¹ 5. NVIDIA Container Toolkit (for Docker/K8s)

* `nvidia-container-cli info` â†’ Show GPU info inside containers
* `nvidia-container-runtime --version` â†’ Check runtime version
* `nvidia-docker run --rm nvidia/cuda:12.2.0-base nvidia-smi` â†’ Run container with GPU support

---

### ðŸ”¹ 6. Other Useful NVIDIA Commands

* `nvidia-settings` â†’ GUI/CLI tool to configure settings (mostly on desktop systems with X server)

  * Example: `nvidia-settings -q GPUCoreTemp` â†’ Query GPU temperature
* `nvidia-bug-report.sh` â†’ Generate diagnostic logs for NVIDIA support
* `nvidia-persistenced` â†’ Daemon to maintain persistence mode

---

### ðŸ”¹ Quick Cheat Examples

```bash
# Monitor GPU every 1s
nvidia-smi -l 1

# Show GPU utilization in CSV
nvidia-smi --query-gpu=index,name,temperature.gpu,utilization.gpu,memory.used,memory.total --format=csv

# Set GPU 0 power limit to 200W
nvidia-smi -i 0 -pl 200

# Kill a GPU process by PID
nvidia-smi --kill-pid=12345
```

---

ðŸ‘‰ This covers **all important NVIDIA GPU-related commands**:

* `nvidia-smi` (monitoring, managing, power control)
* CUDA commands (`nvcc`, `deviceQuery`, profiling)
* NVIDIA Container Toolkit commands
* Debugging/profiling tools (`cuda-gdb`, `nsys`, `cuda-memcheck`)
* Utility (`nvidia-settings`, `nvidia-bug-report.sh`)

---

### ðŸ”¹ 1. `nvidia-smi` Logging Options

* `nvidia-smi -l 1`
  Continuously log GPU stats every 1 second (to screen).
* `nvidia-smi -lms 500`
  Log every 500 milliseconds.
* `nvidia-smi --format=csv --query-gpu=timestamp,utilization.gpu,memory.used,memory.total`
  Output in CSV format (good for logging into a file).
* `nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv`
  Log which processes use GPU memory.

ðŸ‘‰ Redirect output to file:

```bash
nvidia-smi -l 1 --query-gpu=timestamp,utilization.gpu,temperature.gpu,memory.used,memory.total --format=csv > gpu_log.csv
```

---

### ðŸ”¹ 2. NVIDIA Error & Driver Logs

* `/var/log/nvidia-installer.log` â†’ Logs from NVIDIA driver installation
* `/var/log/nvidia-uninstall.log` â†’ Logs from uninstallation
* `/var/log/nvidia-driver.log` â†’ Driver service logs (may vary by distro)
* `/var/log/Xorg.0.log` â†’ Contains NVIDIA driver initialization info (desktop systems)
* `dmesg | grep -i nvidia` â†’ Kernel ring buffer logs (driver events, GPU resets, errors)
* `journalctl -u nvidia-persistenced` â†’ Logs from persistence daemon
* `journalctl -k | grep -i nvidia` â†’ Kernel-related NVIDIA messages

---

### ðŸ”¹ 3. NVIDIA Bug Report Tool

* `nvidia-bug-report.sh`
  Generates a full diagnostic tarball with all NVIDIA-related logs.
  (Very useful for troubleshooting with NVIDIA support.)

---

### ðŸ”¹ 4. Logging GPU Application Performance

* `nvidia-smi dmon`
  Logs real-time GPU utilization, memory, power, clocks.
  Example (log to file every 1s):

  ```bash
  nvidia-smi dmon -s u -d 1 > gpu_usage.log
  ```

  (`-s u` = utilization, `-d 1` = delay 1s)
* `nvidia-smi pmon`
  Per-process monitoring log.

---

### ðŸ”¹ 5. NVIDIA Nsight / Profiling Logs

* `nsys profile -o output_report ./my_app`
  Creates detailed profiling logs.
* `cuda-memcheck ./app`
  Logs CUDA memory errors.
* `cuda-gdb ./app`
  Debugging logs.

---

âœ… **In summary â€“ NVIDIA log-related commands:**

* **Monitoring logs** â†’ `nvidia-smi -l`, `nvidia-smi dmon`, `nvidia-smi pmon`
* **Driver/system logs** â†’ `/var/log/nvidia*.log`, `dmesg | grep nvidia`, `journalctl`
* **Diagnostics** â†’ `nvidia-bug-report.sh`
* **Profiling logs** â†’ `nsys`, `cuda-memcheck`
